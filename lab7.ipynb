{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验七：基于序列模型的中英文翻译机\n",
    "\n",
    "\n",
    "\n",
    "## 7.1 英中机器翻译实验导览\n",
    "\n",
    "\n",
    "\n",
    "本 Notebook 以简单的中英句子翻译任务为例，完整呈现一个机器学习实验从数据准备到模型评估的基本流程，重点面向刚学习完机器学习导论、尚无工程实践经验的同学。\n",
    "\n",
    "\n",
    "\n",
    "### 7.1.1 环境快速检查清单（建议先全部通过再往下走）\n",
    "\n",
    "\n",
    "\n",
    "在开始运行代码前，先按照下面的检查单逐项确认环境是否就绪。若有一步失败，请先解决再继续，以免后面在训练阶段才发现环境问题。\n",
    "\n",
    "\n",
    "\n",
    "**（1）操作系统与 Python 运行环境**\n",
    "\n",
    "\n",
    "\n",
    "- 操作系统：支持 Windows / macOS / Linux，推荐按照课堂示例使用 Linux 或带 VS Code 的桌面环境；\n",
    "\n",
    "- Python 版本：**必须使用 Python 3.8–3.10 之间的版本**（`pzmllab` 暂不支持 3.10 以上版本），可用 `python --version` / `python3 --version` 检查；\n",
    "\n",
    "- 推荐统一使用 **Miniconda** 管理虚拟环境，便于课堂说明和助教协助排查；\n",
    "\n",
    "  - 搜索关键词：`安装 Miniconda`、`conda 创建虚拟环境 python=3.10`；\n",
    "\n",
    "- Jupyter 环境：可以正常启动并打开本 Notebook（推荐在 VS Code 中通过 Jupyter 扩展运行）。\n",
    "\n",
    "\n",
    "\n",
    "**（2）快速安装本实验所需依赖（面向学生）**\n",
    "\n",
    "\n",
    "\n",
    "- 在本仓库 `lab7/` 目录下已经提供精简版依赖文件：`requirements.txt`；\n",
    "\n",
    "- 建议在新建好的虚拟环境中直接执行：\n",
    "\n",
    "  - 使用 pip：`pip install -r requirements.txt`\n",
    "\n",
    "- 该精简文件只包含本实验 Notebook 实际需要的核心包：\n",
    "\n",
    "  - `torch`：用于搭建与训练序列模型；\n",
    "\n",
    "  - `numpy`：基础数值计算；\n",
    "\n",
    "  - `matplotlib`：绘制损失曲线与注意力热力图；\n",
    "\n",
    "  - `nltk`：计算 BLEU 分数；\n",
    "\n",
    "  - `pzmllog`：JetML 实验日志上报（仅在使用 JetML 平台时需要）。\n",
    "\n",
    "- 若安装过程中遇到网络或镜像问题，可搜索：`pip 换国内源`、`conda 环境 配置 教程` 等关键词。\n",
    "\n",
    "\n",
    "\n",
    "> 说明：仓库中可能还包含一个较长的完整依赖快照文件（例如 `requirements.full-freeze.txt`），那是用于老师/助教复现实验环境的参考，不是给学生直接使用的安装列表。\n",
    "\n",
    "\n",
    "\n",
    "**（3）数据与文件路径**\n",
    "\n",
    "\n",
    "\n",
    "- 确认当前工作目录下存在 `./data/eng-cmn.txt`，且编码为 UTF-8；\n",
    "\n",
    "- 若使用 JetML，请确认本地已正确配置 `pzmllog` 所依赖的配置文件 / 服务（详见平台使用说明）；\n",
    "\n",
    "- 若在远程服务器上运行，确保对 `data/` 目录有读权限，对日志 / 模型保存目录有写权限。\n",
    "\n",
    "\n",
    "\n",
    "**（4）GPU / CPU 检查**\n",
    "\n",
    "\n",
    "\n",
    "- 运行本 Notebook 中的“环境检查”代码单元，观察输出的 `device`：\n",
    "\n",
    "  - 若显示 `cuda`，说明已成功识别 GPU；\n",
    "\n",
    "  - 若显示 `cpu`，也可以完成实验，只是训练时间相对更长。\n",
    "\n",
    "- 无论是否有 GPU，本实验都可以在较小迭代次数下完成主要体验。\n",
    "\n",
    "\n",
    "\n",
    "**（5）快速自检：首个代码单元应当能够顺利运行**\n",
    "\n",
    "\n",
    "\n",
    "- 依次运行下方“环境检查、依赖导入”单元：应当可以正常导入所有包，并输出 `device`；\n",
    "\n",
    "- 若在导入某个包时卡住或报错（例如 `ImportError` / `ModuleNotFoundError`），请先根据错误信息安装或修复对应依赖，再继续后续步骤；\n",
    "\n",
    "- 建议在环境稳定后再开始长时间训练，以避免中途因为环境问题中断。\n",
    "\n",
    "\n",
    "\n",
    "**（6）编辑器与阅读体验建议（推荐但非必须）**\n",
    "\n",
    "\n",
    "\n",
    "- 推荐统一使用 **VS Code** 打开本实验仓库，便于与课堂演示保持一致；\n",
    "\n",
    "- 建议安装 VS Code 官方的 Python / Jupyter 扩展（可搜索 `VS Code 安装 Python 插件`）；\n",
    "\n",
    "- 在 VS Code 的“**大纲视图（Outline）**”中可以直接看到本 Notebook 的各级标题结构，便于按照“数据准备→模型→训练→评估”的顺序导航；\n",
    "\n",
    "- 若使用纯浏览器 Jupyter，也可以借助目录扩展（如 Table of Contents）获得类似的大纲视图。\n",
    "\n",
    "\n",
    "\n",
    "### 7.1.2 学习目标\n",
    "\n",
    "\n",
    "\n",
    "完成本实验后，期望你能够：\n",
    "\n",
    "- **数据层面**：理解如何将原始中英文句子经过清洗和编码，转换为模型可以处理的数值表示；\n",
    "\n",
    "- **模型层面**：理解一个“先压缩句子信息、再逐词生成译文”的神经网络结构的整体思路；\n",
    "\n",
    "- **训练层面**：理解通过多轮迭代、损失下降来逐步提升模型性能的基本过程；\n",
    "\n",
    "- **评估层面**：能够结合数值指标和可视化结果，对模型输出质量进行定性与定量分析。\n",
    "\n",
    "\n",
    "\n",
    "### 7.1.3 章节结构概览\n",
    "\n",
    "\n",
    "\n",
    "1. **环境与工具**：导入依赖库，配置日志记录与计时辅助函数；\n",
    "\n",
    "2. **Step1 数据准备**：构建词表，对句子进行标准化处理，读取并筛选语料，统计词频，并通过样例检查数据是否正确；\n",
    "\n",
    "3. **Step2 模型设计**：定义将句子表示为向量的子网络、根据向量逐步生成译文的子网络，以及若干句子与张量之间转换的辅助函数；\n",
    "\n",
    "4. **Step3 模型训练**：给出单步训练函数和多轮训练循环，输出损失值并记录训练过程中的关键信息；\n",
    "\n",
    "5. **Step4 模型评估**：对单句和随机样本进行测试，计算评价分数，并绘制训练曲线与“模型关注位置”的可视化图像；\n",
    "\n",
    "6. **Main Execution**：集中实例化前面定义的组件，**在这一节真正串联并执行“数据准备→模型→训练→评估”这四个流程**；\n",
    "\n",
    "7. **可视化扩展**：调整字体设置，展示多条句子的可视化结果，帮助更直观地理解模型行为。\n",
    "\n",
    "\n",
    "\n",
    "> 提醒：前面的 Step1–Step4 是“搭建和拆解零部件（数据管线、模型结构、训练与评估函数）”，只有在最后的 **Main Execution** 单元里，这些函数才会被实际组合起来并跑完一次完整流程。\n",
    "\n",
    "\n",
    "\n",
    "### 7.1.4 推荐阅读与执行顺序\n",
    "\n",
    "\n",
    "\n",
    "- 首先运行“环境与工具”部分，确保依赖库与运行环境配置正确；\n",
    "\n",
    "- 按照 Step1 中的小节顺序依次执行，直至能够打印出一对中英文句子，确认数据管线工作正常；\n",
    "\n",
    "- 阅读 Step2，理解整体网络结构和数据在其中的流动方式；\n",
    "\n",
    "- 在 Step3 中先进行较少轮次的试验性训练（例如 1,000 次迭代），观察损失值是否呈下降趋势；\n",
    "\n",
    "- 若试验训练结果合理，再进行较大轮次的正式训练（例如 75,000 次迭代）；\n",
    "\n",
    "- 之后进入 Step4 与可视化扩展部分，综合利用分数与图像对模型进行分析与反思。\n",
    "\n",
    "\n",
    "\n",
    "### 7.1.5 运行时间预估（CPU 环境）\n",
    "\n",
    "\n",
    "\n",
    "| 阶段           | 典型耗时   | 说明                         |\n",
    "|----------------|------------|------------------------------|\n",
    "| 数据准备       | < 1 min    | 语料规模较小，统计较为迅速   |\n",
    "| 小规模训练     | 1–2 min    | 用于检查训练过程是否正常     |\n",
    "| 全量训练       | 10–25+ min | 取决于硬件性能和超参数设置   |\n",
    "| 评估与可视化   | 1–3 min    | 生成评价指标与可视化图像     |\n",
    "\n",
    "在具备 GPU 的环境下，训练时间通常会显著缩短。\n",
    "\n",
    "\n",
    "\n",
    "### 7.1.6 初学者操作提示\n",
    "\n",
    "\n",
    "\n",
    "- 逐节运行，避免跳过导致变量未定义；\n",
    "\n",
    "- 调整 `teacher_forcing_ratio` 比较收敛差异 (0 vs 1)；\n",
    "\n",
    "- 多次运行随机评估，观察 BLEU 波动范围；\n",
    "\n",
    "- 注意力若只集中在序列边缘，多训练或增大数据；\n",
    "\n",
    "- 若时间不足：降低迭代次数或隐藏维度。\n",
    "\n",
    "\n",
    "\n",
    "### 7.1.7 可继续探索\n",
    "\n",
    "\n",
    "\n",
    "- 换子词分词 (BPE) 降低未登录词；\n",
    "\n",
    "- 尝试 Transformer 编码器替代 GRU；\n",
    "\n",
    "- 加入学习率调度 / 早停策略；\n",
    "\n",
    "- 比较不同注意力（点积 vs 可加性）。\n",
    "\n",
    "\n",
    "\n",
    "准备好后，进入下一单元开始环境检查。祝实验顺利！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 环境检查、依赖导入、辅助函数声明与实现\n",
    "\n",
    "在动手之前，先把所有会用到的积木准备好：文本处理依赖 `unicodedata`、`re` 负责清洗句子，`torch` 负责搭建与训练 Seq2Seq 模型，`matplotlib` 用于绘图，`nltk` 的 BLEU 分数作为评估指标。运行本单元即可完成环境检查，并确认是否有 GPU 可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:20.709726Z",
     "start_time": "2024-11-01T10:37:17.916732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import jupyter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JetML 平台配置\n",
    "为了便于上传日志、记录超参数，本项目接入了 JetML。下面的代码展示如何设置访问令牌、实验名称与端口，并将学习率、批大小等信息同步出去，方便教学或验收。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Client Connection Error,Check Client's Status Please\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/requests/models.py:976\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/pzmllog/clinet.py:20\u001b[0m, in \u001b[0;36mClinet.__api\u001b[0;34m(self, url, data)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(f\"Send Message:{data}\\n\")\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(f\"Client Response: {msg}\\n\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/requests/models.py:980\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mNewLogger\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#用户 ID\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccess_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr6qo97vwncn4ogiwnsq549bd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#项目 ID\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1863\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#实验描述和说明信息\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#自定义实验名称\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#仓库 ID\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepository_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md6ca066d95d3400fba88a7e9335892ec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# tomcat的启动端口\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5560\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#超参数集\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/pzmllog/mllogger.py:8\u001b[0m, in \u001b[0;36mNewLogger\u001b[0;34m(config, info)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m仅支持python版本 3.6 3.7 3.8 3.9 3.10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[43mLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     l\u001b[38;5;241m.\u001b[39mStart(info)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/pzmllog/utils.py:18\u001b[0m, in \u001b[0;36msingle.<locals>._single\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_single\u001b[39m(\u001b[38;5;241m*\u001b[39margs,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# nonlocal _instance\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _instance :\n\u001b[0;32m---> 18\u001b[0m         _instance[\u001b[38;5;28mcls\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/pzmllog/mylogger.py:21\u001b[0m, in \u001b[0;36mLogger.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__status \u001b[38;5;241m=\u001b[39m NewStatus(config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__client \u001b[38;5;241m=\u001b[39m NewClientConn(config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mShakeHand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__rp_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__cpu \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/pzmllog/clinet.py:36\u001b[0m, in \u001b[0;36mClinet.ShakeHand\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepository_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config:\n\u001b[1;32m     35\u001b[0m     send_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepositoryId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepository_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 36\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__api\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__api_load_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msend_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m NewStoragePath(resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab7/lib/python3.10/site-packages/pzmllog/clinet.py:23\u001b[0m, in \u001b[0;36mClinet.__api\u001b[0;34m(self, url, data)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# print(f\"Client Response: {msg}\\n\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient Connection Error,Check Client\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Status Please\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient Start Successful, But Has Internal Error\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionError\u001b[0m: Client Connection Error,Check Client's Status Please\n"
     ]
    }
   ],
   "source": [
    "from pzmllog import NewLogger\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch = 1\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "log = NewLogger(\n",
    "    config={\n",
    "        #用户 ID\n",
    "        'access_token':\"r6qo97vwncn4ogiwnsq549bd\",\n",
    "        #项目 ID\n",
    "        'project':\"1863\",\n",
    "        #实验描述和说明信息\n",
    "        \"description\":\"test\",\n",
    "        #自定义实验名称\n",
    "        \"experiment_name\":\"TEST\",\n",
    "        #仓库 ID\n",
    "        \"repository_id\":\"d6ca066d95d3400fba88a7e9335892ec\",\n",
    "        # tomcat的启动端口\n",
    "        'port': \"5560\"\n",
    "    },\n",
    "    #超参数集\n",
    "    info = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epoch\": epoch,\n",
    "        \"batch_size\": batch_size\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计时辅助函数\n",
    "长时间训练时需要掌握进度感。下面的工具函数把秒数转换成“分钟 + 秒”，并根据当前迭代估算剩余时间，方便在实验记录中写下训练耗时。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a helper function to print time elapsed and estimated time remaining given the current time and progress %.\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Step1:  数据准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:20.727414Z",
     "start_time": "2024-11-01T10:37:20.712335Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "SOS_token = 0  # Start of Sentence 句子开始标记\n",
    "EOS_token = 1  # End of Sentence 句子结束标记\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"\n",
    "    语言词典类\n",
    "    功能：为一种语言（中文或英文）建立词汇表\n",
    "    - word2index: 单词 → 索引\n",
    "    - index2word: 索引 → 单词\n",
    "    - word2count: 单词出现次数统计\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}  # 单词到索引的映射\n",
    "        self.word2count = {}  # 单词计数\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}  # 索引到单词的映射\n",
    "        self.n_words = 2  # 初始有 2 个词（SOS 和 EOS）\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"添加一个句子中的所有单词到词典\"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"添加单个单词到词典\"\"\"\n",
    "        if word not in self.word2index:\n",
    "            # 新词：分配新索引\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # 已有词：增加计数\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本标准化：把句子清洗到统一格式\n",
    "为了让模型专注在语义而不是标点或大小写差异，我们需要把原始句子逐步变成“全小写、带空格分隔、只保留核心字符”的形式。下面的函数先执行 Unicode 归一化，接着通过正则表达式删掉中英文的句号、感叹号等符号。这一步属于 **数据准备** 环节的第一道工序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:20.743048Z",
     "start_time": "2024-11-01T10:37:20.727414Z"
    }
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "    c for c in unicodedata.normalize('NFD', s)\n",
    "    if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "# 其中normalizeString函数中的正则表达式需对应更改，否则会将中文单词替换成空格\n",
    "def normalizeString(s):\n",
    "    #变成小写，去掉前后空格\n",
    "    s = s.lower().strip()\n",
    "    # 通过有无空格分隔作为分隔符，判断是否为中文（不robust）\n",
    "    if ' ' not in s:\n",
    "        s = list(s)\n",
    "        s = ' '.join(s)\n",
    "    s = unicodeToAscii(s)  #将unicode变成ascii\n",
    "    s = re.sub(r\"([.。!！?？])\", \"\", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取平行语料并构建语言对象\n",
    "`readLangs` 负责打开 `data/eng-cmn.txt`，逐行拆分成英文和中文句子对，再用刚才的标准化函数做清洗。根据 `reverse` 标志可以交换输入/输出语言，从而复用同一份代码处理“英→中”或“中→英”任务。执行下面的代码，即可得到两个 `Lang` 词典对象与原始句子对列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:20.758668Z",
     "start_time": "2024-11-01T10:37:20.743048Z"
    }
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    file_path = \"./data/eng-cmn.txt\"\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子长度筛选：控制学习难度\n",
    "为了让初学者可以在有限算力下快速跑通实验，我们只保留长度不超过 `MAX_LENGTH` 的句子，并聚焦在常见的主谓结构（`eng_prefixes`）。这属于典型的数据清洗策略：先把学习目标收窄，再逐步拓展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:20.774291Z",
     "start_time": "2024-11-01T10:37:20.758668Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 汇总数据准备流程\n",
    "`prepareData` 将前面所有步骤串联：读文件→筛句子→统计词频→生成词典。运行后我们就拿到了后续所有环节共享的数据结构，是“数据准备”阶段的终点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:20.789922Z",
     "start_time": "2024-11-01T10:37:20.774291Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试词典是否正确构建\n",
    "运行下面的单元会实际读取语料、构建词典并随机打印一个句子对。确认能够输出内容后，说明数据准备阶段已经完成，可以继续往模型部分推进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:21.163283Z",
     "start_time": "2024-11-01T10:37:20.789922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 24026 sentence pairs\n",
      "Trimmed to 459 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "cmn 684\n",
      "eng 560\n",
      "['我 正 在 找 一 個 助 手 ', 'i am looking for an assistant']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'cmn', True)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:21.178966Z",
     "start_time": "2024-11-01T10:37:21.163283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file_path = \"./data/eng-cmn.txt\"\\nwith open(file_path, encoding=\\'utf-8\\') as file:\\n    lines = file.readlines()\\npairs = [[normalizeString(l).split(\\'\\t\\')[:2]] for l in lines]\\ncn = []\\neng = []\\nfor p in pairs:\\n    p=np.array(p)\\n    eng.append([p[0,0]])\\n    cn.append([p[0,1]])'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''file_path = \"./data/eng-cmn.txt\"\n",
    "with open(file_path, encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "pairs = [[normalizeString(l).split('\\t')[:2]] for l in lines]\n",
    "cn = []\n",
    "eng = []\n",
    "for p in pairs:\n",
    "    p=np.array(p)\n",
    "    eng.append([p[0,0]])\n",
    "    cn.append([p[0,1]])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Step2:  模型设计\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 RNN Encoder 类：把输入句子压缩成语义向量\n",
    "\n",
    "编码器负责读取“源语言”单词序列并逐步更新隐藏状态，最终把整句的信息压成一个上下文向量。这里使用 `Embedding` + 单层 `GRU`，是理解 Seq2Seq 的最小原型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:21.194593Z",
     "start_time": "2024-11-01T10:37:21.178966Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 RNNDecoder 类：注意力辅助的翻译器\n",
    "\n",
    "解码器一边“看”编码器的输出序列（注意力），一边一步步生成“目标语言”单词。理解解码器的前向过程，有助于你弄清楚“翻译时模型到底在用哪些源单词的信息”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:21.210218Z",
     "start_time": "2024-11-01T10:37:21.194593Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 句子转张量：连接数据与模型\n",
    "\n",
    "本节把前面整理好的句子和词表真正“喂”进 PyTorch：把单词索引组装成张量，并考虑 `SOS/EOS` 等特殊标记。理解这一步有助于你在之后修改模型或替换数据集时少踩坑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:21.225840Z",
     "start_time": "2024-11-01T10:37:21.210218Z"
    }
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Step3:  模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 train() 模型训练函数\n",
    "\n",
    "先从“单次参数更新”出发，拆开看清每一步：前向传播、计算损失、反向传播与参数更新。理解这一小步，有利于你之后阅读 `trainIters` 时不迷路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2 trainIters：把单步训练串成完整流程\n",
    "\n",
    "在前一小节打好“单步训练”基础后，这里通过循环若干 epoch，把训练过程串联起来；你可以在这里观察损失下降趋势，并理解各种训练超参数（学习率、epoch 数等）对结果的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:21.241464Z",
     "start_time": "2024-11-01T10:37:21.225840Z"
    }
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Step4:  模型评估\n",
    "评估阶段分两步：\n",
    "- `evaluate` 输入单个句子，返回预测词序列与注意力矩阵\n",
    "- `evaluateRandomly` 则随机抽样多条句子并计算 BLEU，帮助我们量化模型性能。记得在训练完成后再运行这些单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1 模型评估\n",
    "\n",
    "这里主要通过 BLEU 等指标定量评估翻译质量，并给出若干示例翻译结果，帮助你从“数字”和“具体例句”两个角度理解模型的优缺点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:24.478114Z",
     "start_time": "2024-11-01T10:37:24.462497Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        # log.Log({\"epoch\":epoch,\"loss\":loss,\"accuracy\":accuracy})\n",
    "        log.Log({\"epoch\":epoch,\"loss\":loss})\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2 绘制训练损失曲线\n",
    "\n",
    "最后，我们通过 Matplotlib 把训练过程中的损失变化画成曲线，一眼就能看到模型是否收敛、有没有明显的过拟合/欠拟合趋势，也方便你在之后尝试不同超参数时进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1 绘制训练损失曲线\n",
    "训练完毕后，用 `showPlot` 把损失随迭代的变化画出来，可以快速判断学习率是否合适、是否过拟合。这是“训练”与“评估”之间最直接的桥梁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:24.493743Z",
     "start_time": "2024-11-01T10:37:24.478114Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:37:24.524989Z",
     "start_time": "2024-11-01T10:37:24.509360Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=100):\n",
    "    sum_scores = 0\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        w = []\n",
    "        words = pair[1].strip(' ').split(' ')\n",
    "        words.append('<EOS>')\n",
    "        w.append(words)\n",
    "        bleu_score = sentence_bleu(w, output_words)\n",
    "        sum_scores += bleu_score\n",
    "    print('The bleu_score is ', sum_scores/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Main Execution：执行训练\n",
    "\n",
    "这一单元汇总了“模型 + 数据 + 训练函数”，实例化编码器/解码器后调用 `trainIters`。建议先从较小的 `n_iters` 试跑，确认损失下降后再把迭代次数提升至 75k，以节约时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:51:51.568017Z",
     "start_time": "2024-11-01T10:37:39.727073Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m attn_decoder1 \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# log.Run()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_decoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# trainIters(encoder1, attn_decoder1, 75000, print_every=5000, learning_rate=learning_rate)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# log.End()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 结束整个过程,提交实验结果\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# log.Submit()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 24\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     21\u001b[0m plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# log.Log({\"epoch\":epoch,\"loss\":loss,\"accuracy\":accuracy})\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mlog\u001b[49m\u001b[38;5;241m.\u001b[39mLog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m:epoch,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m:loss})\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m     print_loss_avg \u001b[38;5;241m=\u001b[39m print_loss_total \u001b[38;5;241m/\u001b[39m print_every\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "log.Run()\n",
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "trainIters(encoder1, attn_decoder1, 1000, print_every=100, learning_rate=learning_rate)\n",
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000, learning_rate=learning_rate)\n",
    "log.End()\n",
    "\n",
    "# 结束整个过程,提交实验结果\n",
    "log.Submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1 随机抽取样本进行定性评估\n",
    "\n",
    "训练完后，用 `evaluateRandomly` 抽取若干句子对，手动观察生成结果是否通顺，并结合 BLEU 分数判断模型稳定性。这一步补充了定量指标中可能遗漏的语义问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:51:52.094481Z",
     "start_time": "2024-11-01T10:51:51.568017Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2 绘图字体设置\n",
    "\n",
    "在中文环境下绘图时，Matplotlib 默认字体可能无法显示汉字。以下配置切换到楷体并修复负号显示问题，确保注意力热力图能够正确呈现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:51:52.110176Z",
     "start_time": "2024-11-01T10:51:52.094481Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['KaiTi'] # 指定默认字体\n",
    "plt.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.3 单句注意力热力图示例\n",
    "\n",
    "执行下列代码即可指定一句中文输入、查看模型输出的英文单词，并用 `matshow` 可视化注意力矩阵，帮助理解模型在生成每个词时关注了哪些输入位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:51:52.251197Z",
     "start_time": "2024-11-01T10:51:52.110497Z"
    }
   },
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"你 只 是 玩\")\n",
    "print(output_words)\n",
    "plt.matshow(attentions.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.4 批量可视化多个句子的注意力\n",
    "\n",
    "最后的工具函数封装了“输入句子 → 打印翻译 → 展示注意力热力图”的流程。挑选不同语义结构的句子运行，能直观看出模型在长句、问句等场景下的表现差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:51:52.716117Z",
     "start_time": "2024-11-01T10:51:52.251197Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"他 和 他 的 邻 居 相 处 \")\n",
    "\n",
    "evaluateAndShowAttention(\"我 肯 定 他 会 成 功 的 \")\n",
    "\n",
    "evaluateAndShowAttention(\"他 總 是 忘 記 事 情\")\n",
    "\n",
    "evaluateAndShowAttention(\"我 们 非 常 需 要 食 物 \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
